{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import json, sys\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    # dim  -- number of neurons in the layer\n",
    "    # prev -- prior layer, or None if input layer.  If None, all following\n",
    "    #  params are ignored.\n",
    "    # act -- activation function: accept np.array of z's, return np.array of a's\n",
    "    # act_prime -- derivative function: accept np.arrays of z's and of a's,\n",
    "    #  return derivative of activation wrt z's, 1-D or 2-D as appropriate\n",
    "    # weights -- initial weights. Set to small random if \"None\".\n",
    "    #\n",
    "    # All the following member data are None for input layer\n",
    "    # in_weights -- matrix of input weights\n",
    "    # in_derivs -- derivatives of E/weight for last sample\n",
    "    # zs -- z values for last sample\n",
    "    # z_derivs -- derivatives of E/Z for last sample\n",
    "    # batch_derivs -- cumulative sum of in_derivs across current batch\n",
    "    def __init__(self, dim, prev, act, act_prime, weights=None):\n",
    "        self.dim=dim\n",
    "        self.prev=prev\n",
    "        self.act=act\n",
    "        self.act_prime=act_prime\n",
    "        self.weights=weights\n",
    "    def get_dim(self):\n",
    "        return self.dim\n",
    "    def get_deriv(self, src, trg):\n",
    "    # Compute self.outputs, using vals if given, else using outputs from\n",
    "    # previous layer and passing through our in_weights and activation.\n",
    "    def propagate(self, vals = None):\n",
    "\n",
    "    \n",
    "    # Compute self.in_derivs, assuming \n",
    "    # 1. We have a prev layer (else in_derivs is None)\n",
    "    # 2. Either\n",
    "    #    a. There is a next layer with correct z_derivs, OR\n",
    "    #    b. The provided err_prime function accepts np arrays \n",
    "    #       of outputs and of labels, and returns an np array \n",
    "    #       of dE/da for each output\n",
    "    \"\"\"def backpropagate(self, err_prime=None, labels=None):\n",
    "\n",
    "    # Adjust all weights by avg gradient accumulated for current batch * -|rate|\n",
    "    def apply_batch(self, batch_size, rate):\n",
    "     \n",
    "    # Reset internal data for start of a new batch\n",
    "    def start_batch(self):\n",
    "\n",
    "    # Add delta to the weight from src node in prior layer\n",
    "    # to trg node in this layer.\n",
    "    def tweak_weight(self, src, trg, delta):\n",
    "\n",
    "    # Return string description of self for debugging\n",
    "    def __repr__(self):\n",
    "   \"\"\"\n",
    "class Network:\n",
    "    # arch -- list of (dim, act) pairs\n",
    "    # err -- error function: \"cross_entropy\" or \"mse\"\n",
    "    # wgts -- list of one 2-d np.array per layer in arch\n",
    "    def __init__(self, arch, err, wgts=None):\n",
    "        self.arch = arch\n",
    "        self.err = err\n",
    "        self.wgts = wgts\n",
    "# Forward propagate, passing inputs to first layer, and returning outputs\n",
    "# of final layer\n",
    "\n",
    "\n",
    "#    def predict(self, inputs):\n",
    "\n",
    "\n",
    "# Assuming forward propagation is done, return current error, assuming\n",
    "# expected final layer output is |labels|\n",
    "#    def get_err(self, labels):\n",
    "\n",
    "\n",
    "# Assuming a predict was just done, update all in_derivs, and add to batch_derivs\n",
    "#    def backpropagate(self, labels):\n",
    "\n",
    "\n",
    "# Verify all partial derivatives for weights by adding an\n",
    "# epsilon value to each weight and rerunning prediction to\n",
    "# see if change in error correctly reflects weight change\n",
    "#    def validate_derivs(self, inputs, outputs):\n",
    "\n",
    "\n",
    "# Run a batch, assuming |data| holds input/output pairs comprising the batch\n",
    "# Forward propagate for each input, record error, and backpropagate.  At batch\n",
    "# end, report average error for the batch, and do a derivative update.\n",
    "#    def run_batch(self, data, rate):\n",
    "def load_config(cfg_file):\n",
    "    errors = {\"cross_entropy\": 1, \"mse\": 2}\n",
    "    activations = {\"relu\": 1, \"softmax\": 2}\n",
    "    with open(cfg_file, \"r\") as config:\n",
    "        config_json = json.load(config)\n",
    "        num_layers=len(config_json[\"arch\"])\n",
    "        model = Network(\n",
    "            arch=config_json[\"arch\"],\n",
    "            err=errors[config_json[\"err\"]],\n",
    "            wgts=[np.vstack(config_json.get(\"wgts\")[i]) for i in range(num_layers-1)],\n",
    "        )\n",
    "    return model\n",
    "def load_data(data_file):\n",
    "    with open(data_file, \"r\") as data:\n",
    "        data_json=json.loads(data.read())\n",
    "        input=[data_json[i][0] for i in range(len(data_json))]\n",
    "        input=np.vstack(input)\n",
    "        output=[data_json[i][1] for i in range(len(data_json))]\n",
    "        output=np.vstack(output)\n",
    "        return input,output\n",
    "def main(cmd, cfg_file, data_file):\n",
    "    commands = {\"verify\": 1, \"run\": 2}\n",
    "    # the way this is handled, the strings for the hyperparameters are\n",
    "    # converted to numbers, and used in variables like command and activation\n",
    "    command = commands[cmd]\n",
    "    model=load_config(cfg_file)\n",
    "    input,output=load_data(data_file)\n",
    "main('run', 'CfgEx', 'DataTrain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
