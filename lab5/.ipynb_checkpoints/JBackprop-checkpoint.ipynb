{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import json, sys\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    # dim  -- number of neurons in the layer\n",
    "    # prev -- prior layer, or None if input layer.  If None, all following\n",
    "    #  params are ignored.\n",
    "    # act -- activation function: accept np.array of z's, return np.array of a's\n",
    "    # act_prime -- derivative function: accept np.arrays of z's and of a's,\n",
    "    #  return derivative of activation wrt z's, 1-D or 2-D as appropriate\n",
    "    # weights -- initial weights. Set to small random if \"None\".\n",
    "    # outputs -- output for layer.\n",
    "    # All the following member data are None for input layer\n",
    "    # in_weights -- matrix of input weights\n",
    "    # in_derivs -- derivatives of E/weight for last sample\n",
    "    # zs -- z values for last sample\n",
    "    # z_derivs -- derivatives of E/Z for last sample\n",
    "    # batch_derivs -- cumulative sum of in_derivs across current batch\n",
    "    def __init__(self, dim, prev, act, act_prime, weights=None):\n",
    "        self.dim = dim\n",
    "        self.prev = prev\n",
    "        self.act = act\n",
    "        self.act_prime = act_prime\n",
    "        if self.prev is None:\n",
    "            # input layer\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = (\n",
    "                np.random.uniform(\n",
    "                    low=-0.5, high=0.5, size=(dim, self.prev.dim + 1)\n",
    "                )\n",
    "                if weights is None\n",
    "                else weights\n",
    "            )\n",
    "        # this includes the weights field in the input layer\n",
    "        self.outputs=None\n",
    "    def get_dim(self):\n",
    "        return self.dim\n",
    "\n",
    "    def get_deriv(self, src, trg):\n",
    "        pass\n",
    "\n",
    "    # Compute self.outputs, using vals if given, else using outputs from\n",
    "    # previous layer and passing through our in_weights and activation.\n",
    "    def propagate(self, vals=None):\n",
    "        if vals is not None:\n",
    "            # input layer\n",
    "\n",
    "    # Compute self.in_derivs, assuming\n",
    "    # 1. We have a prev layer (else in_derivs is None)\n",
    "    # 2. Either\n",
    "    #    a. There is a next layer with correct z_derivs, OR\n",
    "    #    b. The provided err_prime function accepts np arrays\n",
    "    #       of outputs and of labels, and returns an np array\n",
    "    #       of dE/da for each output\n",
    "    def backpropagate(self, err_prime=None, labels=None):\n",
    "        pass\n",
    "\n",
    "    # Adjust all weights by avg gradient accumulated for current batch * -|rate|\n",
    "    def apply_batch(self, batch_size, rate):\n",
    "        pass\n",
    "\n",
    "    # Reset internal data for start of a new batch\n",
    "    def start_batch(self):\n",
    "        pass\n",
    "\n",
    "    # Add delta to the weight from src node in prior layer\n",
    "    # to trg node in this layer.\n",
    "    def tweak_weight(self, src, trg, delta):\n",
    "        pass\n",
    "\n",
    "    # Return string description of self for debugging\n",
    "    def __repr__(self):\n",
    "        return \"dim: {!s}\\nact: {}\\nact_prime: {}\\nweights: {!s}\\n\".format(\n",
    "            self.dim, self.act, self.act_prime, self.weights\n",
    "        )\n",
    "\n",
    "\n",
    "class Network:\n",
    "    # arch -- list of (dim, act) pairs\n",
    "    # err -- error function: \"cross_entropy\" or \"mse\"\n",
    "    # wgts -- list of one 2-d np.array per layer in arch\n",
    "    # layers -- list of Layer objects\n",
    "    def __init__(self, arch, err, wgts=None):\n",
    "        layers = []\n",
    "        self.arch = arch\n",
    "        self.err = err\n",
    "        # handling no weights being provided\n",
    "        if wgts == None:\n",
    "            self.wgts = [None]\n",
    "        else:\n",
    "            self.wgts = [None] + wgts\n",
    "        # now to create the random weights if they don't exist.\n",
    "        for layer_no, layer_arch in enumerate(arch):\n",
    "            # layer no 0: input\n",
    "            layers.append(\n",
    "                Layer(\n",
    "                    dim=layer_arch[0],\n",
    "                    prev=None if layer_no == 0 else layers[-1],\n",
    "                    act=layer_arch[1],\n",
    "                    act_prime=layer_arch[1] + \"_prime\",\n",
    "                    weights=None\n",
    "                    if len(self.wgts) < layer_no + 1\n",
    "                    else self.wgts[layer_no],\n",
    "                )\n",
    "            )\n",
    "        self.layers = layers\n",
    "\n",
    "    # Forward propagate, passing inputs to first layer, and returning outputs\n",
    "    # of final layer\n",
    "    def predict(self, inputs):\n",
    "        # for input layer\n",
    "        for layer_no, layer in enumerate(self.layers):\n",
    "            layer.propagate(vals=inputs) if layer_no==0 else layer.propagate()\n",
    "        return None \n",
    "        # now implement Layer.propagate\n",
    "    # Assuming forward propagation is done, return current error, assuming\n",
    "    # expected final layer output is |labels|\n",
    "    def get_err(self, labels):\n",
    "        pass\n",
    "\n",
    "    # Assuming a predict was just done, update all in_derivs, and add to batch_derivs\n",
    "    def backpropagate(self, labels):\n",
    "        pass\n",
    "\n",
    "    # Verify all partial derivatives for weights by adding an\n",
    "    # epsilon value to each weight and rerunning prediction to\n",
    "    # see if change in error correctly reflects weight change\n",
    "    def validate_derivs(self, inputs, outputs):\n",
    "        pass\n",
    "\n",
    "    # Run a batch, assuming |data| holds input/output pairs comprising the batch\n",
    "    # Forward propagate for each input, record error, and backpropagate.  At batch\n",
    "    # end, report average error for the batch, and do a derivative update.\n",
    "    def run_batch(self, data, rate):\n",
    "        inputs = data[\"inputs\"]\n",
    "        outputs = data[\"outputs\"]\n",
    "        for input_no, input in enumerate(inputs):\n",
    "            output=self.predict(input)\n",
    "            print(output)\n",
    "\n",
    "\n",
    "def load_config(cfg_file):\n",
    "    with open(cfg_file, \"r\") as config:\n",
    "        config_json = json.load(config)\n",
    "        arch = config_json[\"arch\"]\n",
    "        err = config_json[\"err\"]\n",
    "        wgts = config_json.get(\"wgts\")\n",
    "        model = Network(arch=arch, err=err, wgts=wgts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_data(data_file):\n",
    "    with open(data_file, \"r\") as data:\n",
    "        data_json = json.loads(data.read())\n",
    "        input = [data_json[i][0] for i in range(len(data_json))]\n",
    "        input = np.vstack(input)\n",
    "        output = [data_json[i][1] for i in range(len(data_json))]\n",
    "        output = np.vstack(output)\n",
    "    return input, output\n",
    "\n",
    "\n",
    "def main(cmd, cfg_file, data_file):\n",
    "    model = load_config(cfg_file)\n",
    "    inputs, outputs = load_data(data_file)\n",
    "    data = {\"inputs\": inputs, \"outputs\": outputs}\n",
    "    model.run_batch(data=data, rate=0.01)\n",
    "\n",
    "main('verify','CfgEx','DataEx')\n",
    "# main(sys.argv[1], sys.argv[2], sys.argv[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
