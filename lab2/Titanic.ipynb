{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massaging the Data\n",
    "Write Python code to convert the Titanic training data into a form suitable for neural network analysis. Pick 7 properties to use, so your input will have shape (*, 7). Several points to consider:\n",
    "\n",
    "Decide how to handle missing age data. This may involve adding another property, which can be one of your 7.\n",
    "Deal with the parsing hassles, especially those surrounding passenger names. Your program should read the original csv from Kaggle, not a hand-tweaked file. This parsing is a basic Python coding problem, but it is exemplary of the \"rusty plumbing\" work that often comes with processing a data set for machine learning.\n",
    "Rework the data into normal distributions with mean 0 and std 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Massage\n",
    "Create a PeopleSet class that stores the values for a Titanic data set of people, either the training set or the test set. PeopleSet's properties include an array of ids, an array of labels (if available) and a tensor of input properties. Each of these have the same length (or 0-dimension length for the tensor) with one entry for each sample. The latter two are ndarrays suitable for use in training a Keras model. PeopleSet has a method that takes a list of CSV file lines of the format provided by Kaggle, and populates its properties by parsing those lines. You may use Python's CSV library, but the parameter passed should still be an iterable list of strings; don't assume in PeopleSet that the data is coming from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeopleSet:\n",
    "    def __init__(self):\n",
    "        self.ids = None\n",
    "        self.labels = None\n",
    "        self.input_properties = None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return ((type(other) == PeopleSet)\n",
    "          and self.ids == other.ids\n",
    "          and self.labels == other.labels\n",
    "          and self.input_properties == other.input_properties\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\"PeopleSet({!r}, {!r}, {!r})\".format(self.ids, self.labels, self.input_properties))\n",
    "    \n",
    "    def populate(self,iterCSV):\n",
    "        inter_csv_list=np.asarray(list(iterCSV))\n",
    "        print(inter_csv_list[np.where(inter_csv_list=='PassengerId')[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      "  'Ticket' 'Fare' 'Cabin' 'Embarked']]\n"
     ]
    }
   ],
   "source": [
    "with open('train.csv') as csvfile:\n",
    "    iterCSV=csv.reader(csvfile)\n",
    "    train=PeopleSet()\n",
    "    train.populate(iterCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
